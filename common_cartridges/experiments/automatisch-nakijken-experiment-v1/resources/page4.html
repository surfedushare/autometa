<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Automatisch nakijken - Meer resultaten en conclusie</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
    <h1>Automatisch nakijken</h1>
    <h2>Meer resultaten en conclusie</h2>
</header>
<main>
    <h3>GPT4.1 (3n voted)</h3>
    <p>Net als met O3-mini wordt met dit model de vraag 3x is gesteld en het meest gegeven antwoord
    wordt gekozen als uiteindelijke uitkomst. Het basis model is echter geen reasoning model, maar is
    speciaal gemaakt voor gebruikers die modellen in combinatie willen gebruiken met andere
    software, zoals bijvoorbeeld in dit experiment om op grote schaal vragen te stellen en te rekenen
    met de uitkomsten.</p>

    <p>Van alle gesteste modellen werkt dit model het beste. Het ontziet de docenten voor ongeveer 2/3
    van de antwoorden. De MAE ligt tussen de 0.3 en 0.7 met een gemiddelde van 0.5. De bias krimpt
    ten opzicht van O3-mini naar 1,2%. Maar belangrijker is dat er voor het eerst een redelijke tot
    goede overeenstemming is met significantie tussen docenten en de LLM voor 3 van de 5 vragen.
    Daarmee is er zeker ruimte voor verbetering, maar het resultaat kan mijns inziens niet worden
    afgedaan als kansloos. De kosten voor een tentamen nakijken zijn ongeveer €30. Een stuk duurder
    dus dan O1-mini, maar nog steeds veel goedkoper dan O1.</p>

    <h3>GPT4.1 (3n voted, human in the loop)</h3>
    <p>Hier is het model en manier van prompten niet veranderd ten opzichte van bovenstaande GPT4.1.
    Echter de manier van scores berekenen is anders om te simuleren dat een docent als extra controle
    naar het toegekende criterium heeft gekeken. Elke keer als een model aangeeft dat een criterium
    "misschien toepasbaar" is, dan wordt de score van dit criterium toegepast mits de score
    overeenkomt met dat van de docent. Dit is om resultaten te simuleren waarbij een docent een
    gedeelte van het werk van de LLM nakijkt waar deze "twijfelt", door met een druk op de knop goed
    te keuren.</p>

    <p>Deze methode van human in the loop is levert echter geen significante verbeteringen op. Het is
    daarom beter om geen gebruik te maken van een onderscheid tussen "toepasbaar" en "misschien
    toepasbaar" bij het vragen naar criteria.</p>

    <h3>GPT4.1 (3n voted, comments review)</h3>
    <p>Ook hier is het model en manier van prompten niet veranderd ten opzichte van bovenstaande
    GPT4.1. Het toekennen van scores werkt zo, dat wanneer een LLM een comment geeft, dat de
    correcte score van de docent wordt overgenomen. In dit geval simuleren we resultaten waarbij een
    docent de comments van een LLM doorkijkt en waar nodig de toegekende criteria verbetert.</p>

    <p>Deze methode is wel veelbelovend. Ongeveer de helft van alle toegekende criteria komt met een
    opmerking van de LLM. Dus het werk dat blindelinks wordt overgenomen verminderd tot 1/3.
    Echter de overeenstemming tussen docent en LLM neemt drastisch toe. De overeenstemming groeit
    naar goed tot uitstekend in 4 van de 5 vragen. De MAE komt tussen 0.1 en 0.4 te liggen met een
    gemiddelde van 0.3. Alleen de bias neemt in negatieve zin toe naar 2,5%, maar blijft dus redelijk.</p>

    <h2>Discussie en volgende stappen</h2>
    <p>Ik heb voor de verschillende GPT4.1modellen gekeken naar waar mogelijke verbeterpunten liggen.
    Vooral bij vraag 4 valt op dat het model consequent een ander criterium kiest dan de docenten, als
    er niet aan comments review wordt gedaan. Het model zou drastisch verbeteren als het lukt de LLM
    iets vaker mee te laten stemmen met de docent, die deze vraag heeft nagekeken (elke vraag is door
    een enkele docent beoordeeld). Als je kijkt naar de opmerkingen bij toegepaste criteria van vraag 4,
    dan valt op dat de LLM consequent oordeelt dat er "onvoldoende terminologie" wordt gebruikt. Dit
    is typisch het soort fout dat verwacht mag worden van LLM's volgens mij. Puur op basis van syntax
    en semantiek is namelijk geen oordeel te vellen over wat "voldoende" is. Mijn vermoeden is dan
    ook, dat als we het antwoordmodel kunnen aanpassen en explicieter maken, dat de LLM dan beter
    de taak zal uitvoeren en handmatige checks van de opmerkingen minder nodig zullen zijn.</p>

    <p>Een mogelijke richting is om een LLM te laten reflecteren op het gegeven antwoord model en
    verbeteringen voor te stellen. Dit zou eventueel wel een duurder "reasoning" model kunnen zijn,
    waarbij een mens de suggesties naloopt en goedkeurt. Het voordeel van deze methode is dat inladen
    van een antwoordmodel wel nog grotendeels automatisch kan gaan en dat het docenten niet veel
    werk hoeft te kosten. Bovendien is er de mogelijkheid om de vragen zelf specifieker te maken,
    wanneer deze nog niet is voorgelegd aan studenten, en dat kan de kwaliteit van nakijken ten goede
    komen. Een mogelijk nadeel van deze methode is dat docenten met weinig prompt ervaring, toch
    nog fouten kunnen maken, die pas tijdens het nakijken zelf zullen blijken.</p>

    <p>Een andere richting is om docenten te laten reageren op opmerkingen waar de LLM mee komt. De
    opmerkingen kunnen dan gebruikt worden om het antwoordmodel te verbeteren. Vervolgens kunnen
    alle antwoorden opnieuw door de LLM gehaald worden, met gebruik van het verbeterde antwoord
    model. Dit proces kan zich herhalen totdat er geen opmerkingen meer zijn. Deze iteratieve methode
    heeft als voordeel, dat de docent continue betrokken is en goed zicht heeft op de gevolgen van
    aanpassingen aan het antwoordmodel. Een nadeel is dat deze methode relatief duur is. Kosten
    kunnen beperkt worden door opmerkingen eerst te laten clusteren. Dit clusteren is vrij goedkoop en
    kan er toe leiden, dat docenten met relatief weinig opmerkingen doornemen, toch veel
    verbeteringen in één keer kunnen aanbrengen. Hoe minder iteraties er nodig zijn om het
    antwoordmodel te verbeteren hoe goedkoper dit proces wordt.</p>

    <h2>Conclusies</h2>
    <p>Het gebruik van LLM als tool bij het nakijken van tentamens lijkt kansrijk. Met relatief weinig
    aanpassingen aan het tentamen kan zo een tool ongeveer 1/3 van het werk uit handen nemen, zonder
    veel op de kwaliteit in te boeten. Vervolg experimenten zouden zich kunnen richten op hoe een
    LLM gebruikt kan worden om het antwoordmodel aan te scherpen voor gebruik bij automatisch
    nakijken. Dit proces van aanscherpen kan vooraf of achteraf het tentamen plaatsvinden, maar vraagt
    in alle gevallen een zekere betrokkenheid van docenten. Daarnaast kan het huidige experiment nog
    uitgebreidt worden met meer verschillende LLM aanbieders. Met name Antropic en Google hebben
    interessante modellen op de markt, die het proberen waard zijn in de toekomst.</p>

    <nav class="page-navigation">
        <div class="prev-page">
            <a href="page3.html">Vorige: Resultaten</a>
        </div>
    </nav>
</main>
<footer>
    <p>Pagina 4 van 4</p>
</footer>
</body>
</html>