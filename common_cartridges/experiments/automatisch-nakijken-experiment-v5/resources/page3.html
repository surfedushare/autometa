<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Automatisch nakijken - Discussie en Conclusies</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
    <h1>Discussie en Conclusies</h1>
</header>
<main>
    <h2>Discussie en volgende stappen</h2>
    <p>Ik heb voor de verschillende GPT4.1modellen gekeken naar waar mogelijke verbeterpunten liggen. Vooral bij vraag 4 valt op dat het model consequent een ander criterium kiest dan de docenten, als er niet aan comments review wordt gedaan. Het model zou drastisch verbeteren als het lukt de LLM iets vaker mee te laten stemmen met de docent, die deze vraag heeft nagekeken (elke vraag is door een enkele docent beoordeeld). Als je kijkt naar de opmerkingen bij toegepaste criteria van vraag 4, dan valt op dat de LLM consequent oordeelt dat er "onvoldoende terminologie" wordt gebruikt. Dit is typisch het soort fout dat verwacht mag worden van LLM's volgens mij. Puur op basis van syntax en semantiek is namelijk geen oordeel te vellen over wat "voldoende" is. Mijn vermoeden is dan ook, dat als we het antwoordmodel kunnen aanpassen en explicieter maken, dat de LLM dan beter de taak zal uitvoeren en handmatige checks van de opmerkingen minder nodig zullen zijn.</p>

    <p>Een mogelijke richting is om een LLM te laten reflecteren op het gegeven antwoord model en verbeteringen voor te stellen. Dit zou eventueel wel een duurder "reasoning" model kunnen zijn, waarbij een mens de suggesties naloopt en goedkeurt. Het voordeel van deze methode is dat inladen van een antwoordmodel wel nog grotendeels automatisch kan gaan en dat het docenten niet veel werk hoeft te kosten. Bovendien is er de mogelijkheid om de vragen zelf specifieker te maken, wanneer deze nog niet is voorgelegd aan studenten, en dat kan de kwaliteit van nakijken ten goede komen. Een mogelijk nadeel van deze methode is dat docenten met weinig prompt ervaring, toch nog fouten kunnen maken, die pas tijdens het nakijken zelf zullen blijken.</p>

    <p>Een andere richting is om docenten te laten reageren op opmerkingen waar de LLM mee komt. De opmerkingen kunnen dan gebruikt worden om het antwoordmodel te verbeteren. Vervolgens kunnen alle antwoorden opnieuw door de LLM gehaald worden, met gebruik van het verbeterde antwoord model. Dit proces kan zich herhalen totdat er geen opmerkingen meer zijn. Deze iteratieve methode heeft als voordeel, dat de docent continue betrokken is en goed zicht heeft op de gevolgen van aanpassingen aan het antwoordmodel. Een nadeel is dat deze methode relatief duur is. Kosten kunnen beperkt worden door opmerkingen eerst te laten clusteren. Dit clusteren is vrij goedkoop en kan er toe leiden, dat docenten met relatief weinig opmerkingen doornemen, toch veel verbeteringen in één keer kunnen aanbrengen. Hoe minder iteraties er nodig zijn om het antwoordmodel te verbeteren hoe goedkoper dit proces wordt.</p>

    <h2>Conclusies</h2>
    <p>Het gebruik van LLM als tool bij het nakijken van tentamens lijkt kansrijk. Met relatief weinig aanpassingen aan het tentamen kan zo een tool ongeveer 1/3 van het werk uit handen nemen, zonder veel op de kwaliteit in te boeten. Vervolg experimenten zouden zich kunnen richten op hoe een LLM gebruikt kan worden om het antwoordmodel aan te scherpen voor gebruik bij automatisch nakijken. Dit proces van aanscherpen kan vooraf of achteraf het tentamen plaatsvinden, maar vraagt in alle gevallen een zekere betrokkenheid van docenten. Daarnaast kan het huidige experiment nog uitgebreidt worden met meer verschillende LLM aanbieders. Met name Antropic en Google hebben interessante modellen op de markt, die het proberen waard zijn in de toekomst.</p>

    <nav class="page-navigation">
        <div class="prev-page">
            <a href="page2.html">Vorige: Resultaten</a>
        </div>
    </nav>
</main>
<footer>
    <p>Pagina 3 van 3</p>
</footer>
</body>
</html>